{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of PPO and A2C in Multi-Agent Environment With Continuous Action Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#env imports\n",
    "import gym\n",
    "from pettingzoo.sisl import multiwalker_v6\n",
    "import supersuit as ss\n",
    "\n",
    "#Algoritm Imports\n",
    "from stable_baselines3 import PPO, A2C\n",
    "from stable_baselines3.ppo import MlpPolicy as ppo_mlp_policy\n",
    "from stable_baselines3.a2c import MlpPolicy as a2c_mlp_policy\n",
    "import torch as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = multiwalker_v6.parallel_env(n_walkers=2,\n",
    "                                  position_noise=1e-3,\n",
    "                                  angle_noise=1e-3,\n",
    "                                  local_ratio=1.0,\n",
    "                                  forward_reward=1.0,\n",
    "                                  terminate_reward=-100.0,\n",
    "                                  fall_reward=-10.0,\n",
    "                                  terminate_on_fall=True,\n",
    "                                  remove_on_fall=True,\n",
    "                                  max_cycles=1000) #create env\n",
    "\n",
    "agents= ['walker_0', 'walker_1'] #name agents\n",
    "\n",
    "#env = ss.frame_stack_v1(env, 4)#stack 4 frames together to see velocity/direction\n",
    "\n",
    "env = ss.pettingzoo_env_to_vec_env_v0(env) #convert to vec env\n",
    "\n",
    "env = ss.concat_vec_envs_v0(env, 1, num_cpus=8, base_class='stable_baselines3') #parallelize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO Model Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "****************************************\n",
      "PPO Model Policy Network: ActorCriticPolicy(\n",
      "  (features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (mlp_extractor): MlpExtractor(\n",
      "    (shared_net): Sequential()\n",
      "    (policy_net): Sequential(\n",
      "      (0): Linear(in_features=31, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "    (value_net): Sequential(\n",
      "      (0): Linear(in_features=31, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (action_net): Linear(in_features=64, out_features=4, bias=True)\n",
      "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "ppo_model = PPO(ppo_mlp_policy, env,  learning_rate=0.0003, n_steps=5, batch_size=10, n_epochs=10, gamma=0.99, gae_lambda=0.95, clip_range=0.2,\n",
    "            clip_range_vf=1, ent_coef=0.0, vf_coef=0.5, max_grad_norm=0.5, use_sde=False, sde_sample_freq=- 1, target_kl=None, \n",
    "            tensorboard_log=None, create_eval_env=False, policy_kwargs=dict(optimizer_class=th.optim.Adam), verbose=0, seed=314, device='auto', _init_setup_model=True)\n",
    "\n",
    "print(f'PPO Model Optimizer: {ppo_model.policy.optimizer_class}')\n",
    "print('*'*40)\n",
    "print(f'PPO Model Policy Network: {ppo_model.policy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A2C Model Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "****************************************\n",
      "A2C Model Policy Network: ActorCriticPolicy(\n",
      "  (features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (mlp_extractor): MlpExtractor(\n",
      "    (shared_net): Sequential()\n",
      "    (policy_net): Sequential(\n",
      "      (0): Linear(in_features=31, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "    (value_net): Sequential(\n",
      "      (0): Linear(in_features=31, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (action_net): Linear(in_features=64, out_features=4, bias=True)\n",
      "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "a2c_model = A2C(a2c_mlp_policy, env,  learning_rate=0.0003, n_steps=5, gamma=0.99, gae_lambda=1.0, ent_coef=0.0, vf_coef=0.5, max_grad_norm=0.5, \n",
    "                use_rms_prop=False, use_sde=False, sde_sample_freq=- 1, normalize_advantage=False, tensorboard_log=None, \n",
    "                create_eval_env=False, policy_kwargs=dict(optimizer_class=th.optim.Adam), verbose=0, seed=314, device='auto', _init_setup_model=True)\n",
    "\n",
    "print(f'A2C Model Optimizer: {a2c_model.policy.optimizer_class}')\n",
    "print('*'*40)\n",
    "print(f'A2C Model Policy Network: {a2c_model.policy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = []\n",
    "for i in range(1386500,(2000000),100000):\n",
    "    steps.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_a2c(training_steps):\n",
    "    for step in training_steps:\n",
    "        a2c_model.learn(total_timesteps = step)\n",
    "\n",
    "        policy = str(step)\n",
    "        a2c_model.save(policy)\n",
    "        print(policy + ' complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(mod, algo):\n",
    "    for i in range(len(steps)):\n",
    "        if i == 0:\n",
    "            mod.learn(total_timesteps = 500)\n",
    "\n",
    "            policy = str(steps[i])\n",
    "            mod.save(policy)\n",
    "        else:\n",
    "            mod = algo.load(str(steps[i-1]), env=env)\n",
    "            mod.learn(total_timesteps = 10000)\n",
    "\n",
    "            policy = str(steps[i])\n",
    "            mod.save(policy)\n",
    "        print(policy + ' complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1386500 complete!\n",
      "1486500 complete!\n",
      "1586500 complete!\n",
      "1686500 complete!\n",
      "1786500 complete!\n",
      "1886500 complete!\n",
      "1986500 complete!\n"
     ]
    }
   ],
   "source": [
    "os.chdir('cont_a2c_log')\n",
    "training_a2c(steps)\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir('cont_ppo_log')\n",
    "#training(ppo_model, PPO)\n",
    "#os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "env1 = multiwalker_v6.parallel_env(n_walkers=2,\n",
    "                                  position_noise=1e-3,\n",
    "                                  angle_noise=1e-3,\n",
    "                                  local_ratio=1.0,\n",
    "                                  forward_reward=1.0,\n",
    "                                  terminate_reward=-100.0,\n",
    "                                  fall_reward=-10.0,\n",
    "                                  terminate_on_fall=True,\n",
    "                                  remove_on_fall=True,\n",
    "                                  max_cycles=1000) #create env\n",
    "\n",
    "agents= ['walker_0', 'walker_1'] #name agents\n",
    "\n",
    "#env = ss.frame_stack_v1(env, 4)#stack 4 frames together to see velocity/direction\n",
    "\n",
    "env1 = ss.pettingzoo_env_to_vec_env_v0(env1) #convert to vec env\n",
    "\n",
    "env1 = ss.concat_vec_envs_v0(env1, 1, num_cpus=8, base_class='stable_baselines3') #parallelize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(model, algo, num_episodes):\n",
    "    \n",
    "    results = pd.DataFrame(columns = ['num_training_steps', 'avg_reward'])\n",
    "    \n",
    "    for count, filename in enumerate(os.listdir()):\n",
    "        \n",
    "        if filename.endswith('.zip'):\n",
    "            print(filename + ' beginning')\n",
    "            mod = model.load(filename.split('.')[0], env=env1)\n",
    "            rewards_per_ep = []\n",
    "            \n",
    "            for ep in range(num_episodes):\n",
    "                rewards = []\n",
    "                steps = 0\n",
    "                obs = env1.reset()\n",
    "                done = np.array([0,0])\n",
    "                \n",
    "                while all(done) != 1:\n",
    "                    action, _states = mod.predict(obs)\n",
    "                    obs, reward, done, info = env1.step(action)\n",
    "                    steps+=1\n",
    "                    rewards.append(reward[0])\n",
    "                    \n",
    "                rewards_per_ep.append(sum(rewards)/len(rewards))\n",
    "            print(filename + ' complete!')\n",
    "                \n",
    "            results = results.append({'num_training_steps' : int(filename.split('.')[0]), 'avg_reward' : sum(rewards_per_ep)/len(rewards_per_ep)}, ignore_index = True)\n",
    "            results.to_csv('results_a2c_cont.csv', index=False)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1155500.zip beginning\n",
      "1155500.zip complete!\n",
      "1287500.zip beginning\n",
      "1287500.zip complete!\n",
      "215000.zip beginning\n",
      "215000.zip complete!\n",
      "1040000.zip beginning\n",
      "1040000.zip complete!\n",
      "264500.zip beginning\n",
      "264500.zip complete!\n",
      "479000.zip beginning\n",
      "479000.zip complete!\n",
      "20500.zip beginning\n",
      "20500.zip complete!\n",
      "165500.zip beginning\n",
      "165500.zip complete!\n",
      "1304000.zip beginning\n",
      "1304000.zip complete!\n",
      "1337000.zip beginning\n",
      "1337000.zip complete!\n",
      "40500.zip beginning\n",
      "40500.zip complete!\n",
      "1370000.zip beginning\n",
      "1370000.zip complete!\n",
      "413000.zip beginning\n",
      "413000.zip complete!\n",
      "1238000.zip beginning\n",
      "1238000.zip complete!\n",
      "1386500.zip beginning\n",
      "1386500.zip complete!\n",
      "693500.zip beginning\n",
      "693500.zip complete!\n",
      "297500.zip beginning\n",
      "297500.zip complete!\n",
      "512000.zip beginning\n",
      "512000.zip complete!\n",
      "957500.zip beginning\n",
      "957500.zip complete!\n",
      "83000.zip beginning\n",
      "83000.zip complete!\n",
      "495500.zip beginning\n",
      "495500.zip complete!\n",
      "182000.zip beginning\n",
      "182000.zip complete!\n",
      "1007000.zip beginning\n",
      "1007000.zip complete!\n",
      "1686500.zip beginning\n",
      "1686500.zip complete!\n",
      "17000.zip beginning\n",
      "17000.zip complete!\n",
      "198500.zip beginning\n",
      "198500.zip complete!\n",
      "231500.zip beginning\n",
      "231500.zip complete!\n",
      "396500.zip beginning\n",
      "396500.zip complete!\n",
      "990500.zip beginning\n",
      "990500.zip complete!\n",
      "726500.zip beginning\n",
      "726500.zip complete!\n",
      "380000.zip beginning\n",
      "380000.zip complete!\n",
      "314000.zip beginning\n",
      "314000.zip complete!\n",
      "363500.zip beginning\n",
      "363500.zip complete!\n",
      "561500.zip beginning\n",
      "561500.zip complete!\n",
      "15500.zip beginning\n",
      "15500.zip complete!\n",
      "611000.zip beginning\n",
      "611000.zip complete!\n",
      "50000.zip beginning\n",
      "50000.zip complete!\n",
      "545000.zip beginning\n",
      "545000.zip complete!\n",
      "1320500.zip beginning\n",
      "1320500.zip complete!\n",
      "132500.zip beginning\n",
      "132500.zip complete!\n",
      "281000.zip beginning\n",
      "281000.zip complete!\n",
      "66500.zip beginning\n",
      "66500.zip complete!\n",
      "446000.zip beginning\n",
      "446000.zip complete!\n",
      "33500.zip beginning\n",
      "33500.zip complete!\n",
      "792500.zip beginning\n",
      "792500.zip complete!\n",
      "809000.zip beginning\n",
      "809000.zip complete!\n",
      "677000.zip beginning\n",
      "677000.zip complete!\n",
      "10500.zip beginning\n",
      "10500.zip complete!\n",
      "1172000.zip beginning\n",
      "1172000.zip complete!\n",
      "594500.zip beginning\n",
      "594500.zip complete!\n",
      "1486500.zip beginning\n",
      "1486500.zip complete!\n",
      "1986500.zip beginning\n",
      "1986500.zip complete!\n",
      "149000.zip beginning\n",
      "149000.zip complete!\n",
      "1271000.zip beginning\n",
      "1271000.zip complete!\n",
      "500.zip beginning\n",
      "500.zip complete!\n",
      "660500.zip beginning\n",
      "660500.zip complete!\n",
      "1089500.zip beginning\n",
      "1089500.zip complete!\n",
      "116000.zip beginning\n",
      "116000.zip complete!\n",
      "858500.zip beginning\n",
      "858500.zip complete!\n",
      "710000.zip beginning\n",
      "710000.zip complete!\n",
      "45500.zip beginning\n",
      "45500.zip complete!\n",
      "429500.zip beginning\n",
      "429500.zip complete!\n",
      "974000.zip beginning\n",
      "974000.zip complete!\n",
      "347000.zip beginning\n",
      "347000.zip complete!\n",
      "743000.zip beginning\n",
      "743000.zip complete!\n",
      "1073000.zip beginning\n",
      "1073000.zip complete!\n",
      "891500.zip beginning\n",
      "891500.zip complete!\n",
      "1122500.zip beginning\n",
      "1122500.zip complete!\n",
      "1205000.zip beginning\n",
      "1205000.zip complete!\n",
      "1188500.zip beginning\n",
      "1188500.zip complete!\n",
      "875000.zip beginning\n",
      "875000.zip complete!\n",
      "1786500.zip beginning\n",
      "1786500.zip complete!\n",
      "842000.zip beginning\n",
      "842000.zip complete!\n",
      "1221500.zip beginning\n",
      "1221500.zip complete!\n",
      "908000.zip beginning\n",
      "908000.zip complete!\n",
      "627500.zip beginning\n",
      "627500.zip complete!\n",
      "1139000.zip beginning\n",
      "1139000.zip complete!\n",
      "30500.zip beginning\n",
      "30500.zip complete!\n",
      "1353500.zip beginning\n",
      "1353500.zip complete!\n",
      "924500.zip beginning\n",
      "924500.zip complete!\n",
      "941000.zip beginning\n",
      "941000.zip complete!\n",
      "759500.zip beginning\n",
      "759500.zip complete!\n",
      "528500.zip beginning\n",
      "528500.zip complete!\n",
      "1023500.zip beginning\n",
      "1023500.zip complete!\n",
      "1586500.zip beginning\n",
      "1586500.zip complete!\n",
      "462500.zip beginning\n",
      "462500.zip complete!\n",
      "248000.zip beginning\n",
      "248000.zip complete!\n",
      "1886500.zip beginning\n",
      "1886500.zip complete!\n",
      "99500.zip beginning\n",
      "99500.zip complete!\n",
      "578000.zip beginning\n",
      "578000.zip complete!\n",
      "330500.zip beginning\n",
      "330500.zip complete!\n",
      "1254500.zip beginning\n",
      "1254500.zip complete!\n",
      "60500.zip beginning\n",
      "60500.zip complete!\n",
      "776000.zip beginning\n",
      "776000.zip complete!\n",
      "825500.zip beginning\n",
      "825500.zip complete!\n",
      "1056500.zip beginning\n",
      "1056500.zip complete!\n",
      "1106000.zip beginning\n",
      "1106000.zip complete!\n",
      "644000.zip beginning\n",
      "644000.zip complete!\n"
     ]
    }
   ],
   "source": [
    "os.chdir('cont_a2c_log')\n",
    "results_a2c_continuous = testing(a2c_model, A2C, 100)\n",
    "results_a2c_continuous.sort_values(by=['num_training_steps'])\n",
    "os.chdir('..')\n",
    "results_a2c_continuous.to_csv('results_a2c_cont.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir('cont_ppo_log')\n",
    "#results_ppo_continuous = testing(ppo_model, PPO, 100)\n",
    "#results_ppo_continuous.sort_values(by=['num_training_steps'])\n",
    "#os.chdir('..')\n",
    "#results_ppo_continuous.to_csv('results_ppo_cont.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ppo_continuous = pd.read_csv('results_ppo_cont.csv')\n",
    "results_ppo_continuous['algorithm'] = 'PPO'\n",
    "\n",
    "results_a2c_continuous = pd.read_csv('results_a2c_cont.csv')\n",
    "results_a2c_continuous['algorithm'] = 'A2C'\n",
    "\n",
    "results_continuous = pd.concat([results_ppo_continuous, results_a2c_continuous])\n",
    "results_continuous.to_csv('results_continuous.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5559",
   "language": "python",
   "name": "ds5559"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
